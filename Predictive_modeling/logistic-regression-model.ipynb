{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Logistic Regression Prediction Model\n# Data set: Telecom company churn data\n# Sean Pharris\n# Jan 12, 2021","metadata":{}},{"cell_type":"markdown","source":"## Part I:\n\n### A1.  What features of provided service have a higher likelihood to cause churn?\n\n\n### A2.  The goal of identifying features that are likely to cause the discontinue of service will help realize aspects of the service that might need to be altered. By using the historical data of already disontinued customers, we can predict the features causing churn with the available data. ","metadata":{}},{"cell_type":"markdown","source":"## Part II:\n\n### B1.  Summarize the assumptions of a logistic regression model.\n - The response variable will be binary\n - The observations will independent of each other\n - There will be no multicollinearity\n - There will be no extreme outliers\n - There will be a obvious relationship between the independent and dependent variables\n - The data set is large enough to draw conclusions from","metadata":{}},{"cell_type":"markdown","source":"### B2.  The benefits of Python are vast but the main reason are the versatility, ease of use, and strong support from the community. There are many packages that make it easy to undertake the task of doing data analysis/data prediction.\nSome of those packages are:  \n   - Pandas and Numpy - make it easy to handle large sets of data\n   - Seaborn and Matplotlib - make data visualization a breeze\n   - Statsmodels and ScikitLearn - allow for easy data exploration and prediction","metadata":{}},{"cell_type":"markdown","source":"### B3.  Logistic regression is an appropriate technique to analyze the research question because our target variable, predicting what features are likely to cause customers to discontinue their services, is a continuous variable. There are several explanatory variables that will help us understand when trying to predict how many customers will discontiue their services. When adding or removing independent variables from our regression analysis, we will find out whether or not they have a positive or negative relationship to the target variable.","metadata":{}},{"cell_type":"markdown","source":"## Part III:\n\n### C1.  The steps to prepare the data/manipulate the data to achieve the necessary goals are:\n        -Limiting the amount of columns to only the necessary columns\n        -Changing any column names to make understanding easier\n        -Ensure we have no null values\n        -Encoding the categorical columns to for the logistic regression model\n        -Eliminating outliers in the numerical variables\n\n     After this is completed, the data will be prepared for analysis.","metadata":{}},{"cell_type":"markdown","source":"### C2.  \nOur independent variables will go under analysis to limit them to only the necessary variables. Finding those variables will help us determine the probabiliy of customer churn (our research question) and ensure us that it will be an accurate model. The \"Churn\" variable will our target variable and will allow us to determine what future customers will discontinue their services as well. \"Churn\" is defined in the data dictionary as, \"Whether the customer discontinued service within the last month\" and we plan to use a large portion of the other variables to determine what makes a customer have the desire to discontinue their services. \n\nPredictor variables:   \n        'Population', 'Area', 'TimeZone', 'Children', 'Age', 'Income',\n       'Marital', 'Gender', 'Churn', 'Outage_sec_perweek', 'Email', 'Contacts',\n       'Yearly_equip_failure', 'Techie', 'Contract', 'Port_modem', 'Tablet',\n       'InternetService', 'Phone', 'Multiple', 'OnlineSecurity',\n       'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n       'StreamingMovies', 'PaperlessBilling', 'PaymentMethod', 'Tenure',\n       'MonthlyCharge', 'Bandwidth_GB_Year', 'TimelyResponse', 'TimelyFixes',\n       'TimelyReplacements', 'Reliability', 'Options', 'RespectfulResponse',\n       'CourteousExchange', 'EvidenceOfActiveListening'  \nTarget:   \n        'Churn'","metadata":{}},{"cell_type":"markdown","source":"### C3.  The steps to prepare the data are:\n 1. Read the data into the data frame (\"df\") using Pandas \"read_csv()\"\n 2. Drop unneeded columns\n 3. Changing the names of columns to make the data more understandable\n 4. Make sure there are no null values\n 5. Create dummy variables for categorical columns\n 6. Remove the outliers of numerical data types\n\n The code is below","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"# Read in data set into the data frame \ndf = pd.read_csv('../input/clean-churn-data/churn_clean.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the data\n\n## Changing the name of columns to make the data more understandable","metadata":{}},{"cell_type":"code","source":"# Renaming the survey columns\ndf.rename(columns = {'Item1':'TimelyResponse', \n                    'Item2':'TimelyFixes', \n                     'Item3':'TimelyReplacements', \n                     'Item4':'Reliability', \n                     'Item5':'Options', \n                     'Item6':'RespectfulResponse', \n                     'Item7':'CourteousExchange', \n                     'Item8':'EvidenceOfActiveListening'}, \n          inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View all columns\ndf.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removing unneeded columns","metadata":{}},{"cell_type":"code","source":"# Drop unnecessary columns\ndf.drop(columns=['CaseOrder','UID', 'Customer_id','Interaction', 'Job','State','City','County','Zip','Lat','Lng'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the data set\nfor col in df:\n    print(col, \":\", df[col].unique(), \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View data types of our variables\ndf.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find all categorical columns so we can determine which ones will need encoding","metadata":{}},{"cell_type":"code","source":"# find categorical variables\n\ncategorical = [var for var in df.columns if df[var].dtype=='O']\n\nprint('There are {} categorical variables\\n'.format(len(categorical)))\n\nprint('The categorical variables are :', categorical)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view the categorical variables\n\nprint(categorical)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make sure the categorical data types do not have any null values","metadata":{}},{"cell_type":"code","source":"# check missing values in categorical variables\n\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view frequency distribution of categorical variables\nfor var in df:\n    print(var, \"\\n\")\n    print(df[var].value_counts(), \"\\n\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for var in df: \n    print(var, \"\\n\")\n    print(df[var].unique(), \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for var in df: \n    print(df[var].value_counts()/np.float(len(df)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for cardinality in categorical variables\n\nfor var in df:\n    print(var, ' contains ', len(df[var].unique()), ' labels')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding the numerical data types so we can remove the outliers","metadata":{}},{"cell_type":"code","source":"# find numerical variables\n\nnumerical = [var for var in df.columns if df[var].dtype!='O']\n\nprint('There are {} numerical variables\\n'.format(len(numerical)))\n\nprint('The numerical variables are :', numerical)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view the numerical variables\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the numerical data types for null values","metadata":{}},{"cell_type":"code","source":"# check missing values in numerical variables\n\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conduct univariate visualization on our numerical data types ","metadata":{}},{"cell_type":"code","source":"# Univariate visualizations of the independent variables\n\nfrom matplotlib import pyplot as plt\n\nfor col in df[numerical]:\n    df[[col]].hist()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view summary statistics in numerical variables\n\nprint(round(df.describe()),2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variables that may contain outliers:  \n- Population\n- Income\n- Bandwidth_GB_Year","metadata":{}},{"cell_type":"code","source":"# draw boxplots to visualize outliers\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15,10))\n\n\nplt.subplot(2, 2, 1)\nfig = df.boxplot(column='Population')\nfig.set_title('')\nfig.set_ylabel('Population')\n\n\nplt.subplot(2, 2, 2)\nfig = df.boxplot(column='Income')\nfig.set_title('')\nfig.set_ylabel('Income')\n\n\nplt.subplot(2, 2, 3)\nfig = df.boxplot(column='Bandwidth_GB_Year')\nfig.set_title('')\nfig.set_ylabel('Bandwidth_GB_Year')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bivariate visualization of distribution","metadata":{}},{"cell_type":"code","source":"# plot histogram to check distribution\n\nplt.figure(figsize=(15,10))\n\n\nplt.subplot(2, 2, 1)\nfig = df.Population.hist(bins=10)\nfig.set_xlabel('Population')\nfig.set_ylabel('Churn')\n\n\nplt.subplot(2, 2, 2)\nfig = df.Income.hist(bins=10)\nfig.set_xlabel('Income')\nfig.set_ylabel('Churn')\n\n\nplt.subplot(2, 2, 3)\nfig = df.Bandwidth_GB_Year.hist(bins=10)\nfig.set_xlabel('Bandwidth_GB_Year')\nfig.set_ylabel('Churn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removing the outliers in our numerical data types\n - Bandwidth_GB_Year does not appear to be skewed.\n - Population and income apprear to be skewed so we will conduct an interquantile range now.","metadata":{}},{"cell_type":"code","source":"# find outliers for Population\n\nIQR = df.Population.quantile(0.75) - df.Population.quantile(0.25)\nlower = df.Population.quantile(0.25) - (IQR * 3)\nupper = df.Population.quantile(0.75) + (IQR * 3)\nprint('Population outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=lower, upperboundary=upper))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find outliers for Income\n\nIQR = df.Income.quantile(0.75) - df.Income.quantile(0.25)\nlower = df.Income.quantile(0.25) - (IQR * 3)\nupper = df.Income.quantile(0.75) + (IQR * 3)\nprint('Income outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=lower, upperboundary=upper))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fixing the outliers in our numerical data types\nWe have seen that the Population and Income columns contain outliers. I will use top-coding approach to cap maximum values and remove outliers from the above variables.","metadata":{}},{"cell_type":"code","source":"def max_value(df3, variable, top):\n    return np.where(df3[variable]>top, top, df3[variable])\n\nfor df3 in [df]:\n    df3['Population'] = max_value(df3, 'Population', 50458.0)\n    df3['Income'] = max_value(df3, 'Income', 155310.5275)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.Population.max(), df.Income.max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot histogram to check distribution of removed outliers \n\nplt.figure(figsize=(15,10))\n\n\nplt.subplot(2, 2, 1)\nfig = df.Population.hist(bins=10)\nfig.set_xlabel('Population')\nfig.set_ylabel('Churn')\n\n\nplt.subplot(2, 2, 2)\nfig = df.Income.hist(bins=10)\nfig.set_xlabel('Income')\nfig.set_ylabel('Churn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## C4.  Generate univariate and bivariate visualizations of the distributions of variables in the cleaned data set.\n\nCode and plots can be seen above.","metadata":{}},{"cell_type":"markdown","source":"## C5.  Provide a copy of the prepared data set.","metadata":{}},{"cell_type":"code","source":"# Desired data set\ndf.to_csv('logistic_regression_churn.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part IV:\n\n### D1.  Construct an initial logistic regression model from all predictors that were identified in Part C2.","metadata":{}},{"cell_type":"markdown","source":"## Initial model","metadata":{}},{"cell_type":"code","source":"# create target(predictor) variable \n\nX = df.drop(['Churn'], axis=1)\n\ny = df['Churn']\n\n# split X and y into training and testing sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing churn from categorical list because we will loop through the categorical in the encoding below\n\ncategorical.remove('Churn')\n\ncategorical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode categorical with 1s and 0s so we can analyze it in a logistic regression\n\nfrom sklearn import preprocessing\n\nfor feature in categorical:\n    le = preprocessing.LabelEncoder()\n    X_train.loc[:, feature] = le.fit_transform(X_train.loc[:, feature])\n    X_test.loc[:, feature] = le.transform(X_test.loc[:, feature])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalizaing/feature scaling the data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model with our training data\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the results and get accuracy of the model\n\ny_pred = logreg.predict(X_test)\n\nprint('Model accuracy: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check null accuracy score\n\nnull_accuracy = (1486/(1486+514))\n\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot histogram of predicted probabilities\n\n\n# adjust the font size \nplt.rcParams['font.size'] = 12\n\n\n# plot histogram with 10 bins\nplt.hist(y_pred, bins = 10)\n\n\n# set the title of predicted probabilities\nplt.title('Histogram of predicted customer churn')\n\n\n# set the x-axis limit\nplt.xlim(0,1)\n\n\n# set the title\nplt.xlabel('Predicted probability of customer churn')\nplt.ylabel('Frequency')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conduct a PCA to reduce the model","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA()\n\nX_train = pca.fit_transform(X_train)\n\ncount = 0\nfor var in pca.explained_variance_ratio_:\n    count += 1\n    print(count, \":\", var)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,38,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The above analysis show us that the first 30 components explain roughly 90% of the variance","metadata":{}},{"cell_type":"code","source":"# print out the PCA variables for the reduced model with their explained variance \n\ncount = 0\ntotal_ev = 0\nwhile count < 30:\n    print(count, X.columns[count], \": \",  pca.explained_variance_ratio_[count])\n    total_ev += pca.explained_variance_ratio_[count]\n    count += 1\n   \nprint(\"The above variables have a sum to equal a {0:0.4f} explained variance.\".format( total_ev))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('To be removed:')\nprint(X.columns[30:38])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## D2.  Reduce the model\n\n    To reduce the model, a Principal Component Analysis was conducted to find explained variances in the variables. After conducting the analysis we can see that the first 30 of 38 variables explain just over 90% of the variance. We can reduce the model to from 38 to 30 and still have the same accuracy.","metadata":{}},{"cell_type":"code","source":"# reduce the number of variables for the reduced model\n\nX = df.drop(['TimelyResponse', 'TimelyFixes','TimelyReplacements','Reliability','Options','RespectfulResponse', 'CourteousExchange',\n       'EvidenceOfActiveListening', 'Churn'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recreate target(predictor) variable \n\ny = df[['Churn']]\n\n# split X and y into training and testing sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical = [var for var in X.columns if X[var].dtype=='O']\n\nfor feature in categorical:\n    le = preprocessing.LabelEncoder()\n    X_train.loc[:, feature] = le.fit_transform(X_train.loc[:, feature])\n    X_test.loc[:, feature] = le.transform(X_test.loc[:, feature])\n\ny_train.loc[:, 'Churn'] = le.fit_transform(y_train.loc[:, 'Churn'])\ny_test.loc[:, 'Churn'] = le.transform(y_test.loc[:, 'Churn'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## D3.  Reduced logistic regression model that includes both categorical and continuous variables.","metadata":{}},{"cell_type":"code","source":"X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = logreg.predict(X_test)\n\nprint('Model accuracy: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\n\n\npredictions = logreg.predict(X_test)\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(accuracy_score(y_test, predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test,predictions)))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot histogram of predicted probabilities\n\n\n# adjust the font size \nplt.rcParams['font.size'] = 12\n\n\n# plot histogram with 10 bins\nplt.hist(y_pred, bins = 10)\n\n\n# set the title of predicted probabilities\nplt.title('Histogram of predicted customer churn')\n\n\n# set the x-axis limit\nplt.xlim(0,1)\n\n\n# set the title\nplt.xlabel('Predicted probability of customer churn')\nplt.ylabel('Frequency')\n\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observations  \n - We can see that the above histogram is highly positive skewed.\n - The first column tell us that there are approximately 1500 observations with probability between 0.0 and 0.1.\n - Roughly 500 observations with probabiliy between .9 and 1\n - Majority of observations predict that there will be no customer churn.","metadata":{}},{"cell_type":"code","source":"X.drop(columns=['Area','TimeZone', 'Marital','PaymentMethod'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical = [var for var in X.columns if X[var].dtype=='O']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Changing all variables into numerical to be plotted in residual plot\n\nfor var in X[categorical]:\n    if len(X[var].unique()) == 2:\n        X[var].replace(('Yes', 'No'), (1, 0), inplace=True)\n    elif len(X[var].unique()) == 3:\n        X[var].replace((X[var].unique()[0], X[var].unique()[1], X[var].unique()[2]), (1, 0, 0), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# turn data types to int so all variables can be plotted\nX = X.astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making \"Churn\" binary numerical\ny['Churn'].replace(('Yes', 'No'), (1, 0), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport statsmodels.api as api\n\nreduced_model = api.OLS(y['Churn'], X[['Population', 'Children', 'Age', 'Income',\n       'Gender', 'Outage_sec_perweek', 'Email', 'Contacts',\n       'Yearly_equip_failure', 'Techie', 'Contract', 'Port_modem', 'Tablet',\n       'InternetService', 'Phone', 'Multiple', 'OnlineSecurity',\n       'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n       'StreamingMovies', 'PaperlessBilling', 'Tenure',\n       'MonthlyCharge', 'Bandwidth_GB_Year']]).fit()\n\n# Residual plot\nresidual_plot = y['Churn'] - reduced_model.predict(X[['Population', 'Children', 'Age', 'Income',\n       'Gender', 'Outage_sec_perweek', 'Email', 'Contacts',\n       'Yearly_equip_failure', 'Techie', 'Contract', 'Port_modem', 'Tablet',\n       'InternetService', 'Phone', 'Multiple', 'OnlineSecurity',\n       'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n       'StreamingMovies', 'PaperlessBilling', 'Tenure',\n       'MonthlyCharge', 'Bandwidth_GB_Year']])\nsns.residplot(x=y['Churn'],y=residual_plot)\nplt.xlabel('Churn')\nplt.ylabel('Residual Plots')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Churn'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.discrete.discrete_model as sm\n\nX['intercept'] = 1\n\nmodel = sm.Logit(y['Churn'], X[['intercept', 'Population', 'Children', 'Age', 'Income',\n       'Gender', 'Outage_sec_perweek', 'Email', 'Contacts',\n       'Yearly_equip_failure', 'Techie', 'Contract', 'Port_modem', 'Tablet',\n       'InternetService', 'Phone', 'Multiple', 'OnlineSecurity',\n       'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n       'StreamingMovies', 'PaperlessBilling', 'Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year']])\nresult = model.fit()\nresult.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## E1.  Initial/Reduced model comparison\n* The variable selection technique conducted to come up with the reduced model consisted of:\n    * Conducting PCA\n    * Finding the explained variance for each variable\n    * Find the amount of variables that made up 90% of the explained variance\n    * Reducing the model to only those variables\n        \n* Initially we started out with 38 variables and after conducting our principal component analysis we were left with only 30 variables.\n      The explained variance of these 30 variables gave us a sum of 0.9092 (90.92%). \n      \n* The initial model had an accuracy of  0.8850 and after reducing the model, we had an accuracy of 0.8825, so we can see that our model is almost just as accurate.\n     \n* Residual plot is above.\n\n\n## E2/E3. All code and outputs are above.\n\n\n\n## Part V: \n\n### F1.  Results:\n* Interpretation of coefficients:\n    * Below are the list coefficients from our logistic regression of the reduced model. These coefficients indicate the odds of causing customer churn. Those with high numbers have a higher probabiliy of causing customer churn. \n    * Example:\n        * Streaming movies has a coefficient of 2.3, which means it is twice as likely that if the customer has the streaming movie service.\n    \n    Population              0.999998  \n    Children                0.951795  \n    Age                     1.006340  \n    Income                  1.000001  \n    Gender                  1.090807  \n    Outage_sec_perweek      0.995896  \n    Email                   0.996673  \n    Contacts                1.046499  \n    Yearly_equip_failure    0.964460  \n    Techie                  2.318347  \n    Contract                0.121443  \n    Port_modem              1.105755  \n    Tablet                  0.952015  \n    InternetService         0.370797  \n    Phone                   0.811606  \n    Multiple                1.173875  \n    OnlineSecurity          0.687936  \n    OnlineBackup            0.828438  \n    DeviceProtection        0.869035  \n    TechSupport             0.847389  \n    StreamingTV             1.930878  \n    StreamingMovies         2.309543  \n    PaperlessBilling        1.100267  \n    Tenure                  0.806118  \n    MonthlyCharge           1.030744  \n    Bandwidth_GB_Year       1.001495  \n    \n* Logistic regression equation:    \n    \n    * For customer churn the probabiliy is 2650 (yes) / 7350 (no) = 0.36 = P\n\n    * The likelihood for a feature to cause churn = P + (coefficient * feature)\n    \n\n* Significance of the model:\n    * From the model we can see that certain features of the service have a higher likelihood of causing customer churn.\n    * Those variables are:\n        * Techie - More than twice as likely to cause customer churn\n        * StreamingTV - More than twice as likely to cause customer churn\n        * StreamingMovies - Almost twice as likely to cause customer churn\n    * With this information we can identify features that need to be reassessed. The question could be asked, \"why do these features cause higher customer churn than the others\"?\n    \n* Limitations:\n    * If we had the historical data of the customers for several years we could evaluate the features as they were implemented into the service. Such as, before Streaming TV was available, what was the feature with the highest likelihood of causing customer churn? \n\n### F2.  Recommendation\n* Based off of the finding in the data analysis, I would recommend the company take a deeper dive into why these specific features of the service have a higher likelihood to cause customer churn and perhaps that would slow down the rate at which customers discontinue their services.\n\n\nPart VI:\n\n### G.  Panopto video recording:\nhttps://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=5e5015ed-c664-4b8a-a2d8-ae1b017a29a9\n\n\n### H.  Sources for Third Party Code:\n\nhttps://www.kaggle.com/prashant111/eda-logistic-regression-pca  \nhttps://www.kaggle.com/prashant111/logistic-regression-classifier-tutorial  \nhttps://www.edureka.co/blog/logistic-regression-in-python/\n\n### I.  References:\n\n\n","metadata":{}}]}