{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sean Pharris\n# Model: Decision Tree\n# Data set: Customer data of a telecommunications company\n# Date: Jan 21, 2022","metadata":{}},{"cell_type":"markdown","source":"Part I: Research Question\n\nA.\n\n1.  What customers are likely to discontinue their services in the next few months?\n\n2.  The goal of identifying the results to our research question can help the stakeholders to understand the turn over of customers in detail.\n \n\nPart II: Method Justification\n\nB.  \n\n1.  Prediction method and outcomes:\n* We will be making the prediction with the Decision Tree algorithim. This algorithim find the statistical significance between the differences between sub-nodes (service features) and parent node (churn). We will measure it by the sum of squares of standardized differences between observed and expected frequencies of the target variable. It works with the categorical target variable of “yes” or “no”. Higher the value of the statistical significance of differences between sub-node and Parent node. With the outcome, we can find predict which customers are likely to discontinue their services in the next few months.\n\n\n2.  Assumptions: \n* Initially, the whole training set is considered as the root (Chauhan, N. (2020)).\n\n* Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model (Chauhan, N. (2020)).\n\n* Records are distributed recursively on the basis of attribute values (Chauhan, N. (2020)).\n\n* Order to placing attributes as root or internal node of the tree is done by using some statistical approach (Chauhan, N. (2020)).\n\n3.  The benefits of Python are vast but the main reason are the versatility, ease of use, and strong support from the community. There are many packages that make it easy to undertake the task of doing data analysis/data prediction.\n\n* Some of those packages are:\n    * Pandas and Numpy - make it easy to handle large sets of data\n    * Seaborn and Matplotlib - make data visualization a breeze\n    * Statsmodels and ScikitLearn - allow for easy data exploration and prediction\n \n\nPart III: Data Preparation\n\nC.  \n\n1.  The customers that have already discontinued their services in the last month have the binary variable as \"yes\" in the \"Churn\" column and \"no\" for those customers that have no discontinued their services. We will preprocess the data as 1s for \"yes\" and 0s for \"no\" to process the data..\n\n2.  The initial data set will include the variables below and our dependent variable will be \"Churn\", which is categorical.\n\n* Continuous:\n\n    'Population', 'Children', 'Age', 'Income', 'Outage_sec_perweek', 'Email', 'Contacts', 'Yearly_equip_failure', 'Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year', 'TimelyResponse', 'TimelyFixes', 'TimelyReplacements', 'Reliability', 'Options', 'RespectfulResponse', 'CourteousExchange', 'EvidenceOfActiveListening'\n       \n* Categorical: \n\n    'Area', 'TimeZone', 'Job', 'Marital', 'Gender', 'Techie', 'Contract', 'Port_modem', 'Tablet', 'InternetService', 'Phone', 'Multiple', 'OnlineSecurity','OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'PaymentMethod'\n\n3.  Steps to prepare data:\n    1. Read the data into the data frame (\"df\") using Pandas \"read_csv()\"\n    2. Drop unneeded columns\n    3. Changing the names of columns to make the data more understandable\n    4. Make sure there are no null values\n    5. Create dummy variables for categorical columns\n    6. Remove the outliers of numerical data types","metadata":{}},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:40.967094Z","iopub.execute_input":"2022-01-21T14:18:40.967658Z","iopub.status.idle":"2022-01-21T14:18:40.973514Z","shell.execute_reply.started":"2022-01-21T14:18:40.96759Z","shell.execute_reply":"2022-01-21T14:18:40.972658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-21T14:18:40.975303Z","iopub.execute_input":"2022-01-21T14:18:40.975678Z","iopub.status.idle":"2022-01-21T14:18:40.991572Z","shell.execute_reply.started":"2022-01-21T14:18:40.975645Z","shell.execute_reply":"2022-01-21T14:18:40.990156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import the data","metadata":{}},{"cell_type":"code","source":"# Read in data set into the data frame \ndf = pd.read_csv('../input/clean-churn-data/churn_clean.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:40.993131Z","iopub.execute_input":"2022-01-21T14:18:40.993794Z","iopub.status.idle":"2022-01-21T14:18:41.148401Z","shell.execute_reply.started":"2022-01-21T14:18:40.993736Z","shell.execute_reply":"2022-01-21T14:18:41.147277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing unneeded columns","metadata":{}},{"cell_type":"code","source":"# Drop unnecessary columns\ndf.drop(columns=['CaseOrder','UID', 'Customer_id','Interaction', 'Job','State','City','County','Zip','Lat','Lng', 'TimeZone', 'Marital'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:41.151484Z","iopub.execute_input":"2022-01-21T14:18:41.152263Z","iopub.status.idle":"2022-01-21T14:18:41.166803Z","shell.execute_reply.started":"2022-01-21T14:18:41.152203Z","shell.execute_reply":"2022-01-21T14:18:41.165843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Changing the name of columns to make the data more understandable","metadata":{}},{"cell_type":"code","source":"# Renaming the survey columns\ndf.rename(columns = {'Item1':'TimelyResponse', \n                    'Item2':'TimelyFixes', \n                     'Item3':'TimelyReplacements', \n                     'Item4':'Reliability', \n                     'Item5':'Options', \n                     'Item6':'RespectfulResponse', \n                     'Item7':'CourteousExchange', \n                     'Item8':'EvidenceOfActiveListening'}, \n          inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:41.168271Z","iopub.execute_input":"2022-01-21T14:18:41.168856Z","iopub.status.idle":"2022-01-21T14:18:41.176147Z","shell.execute_reply.started":"2022-01-21T14:18:41.168808Z","shell.execute_reply":"2022-01-21T14:18:41.175363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check for null values in the data set","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:41.177829Z","iopub.execute_input":"2022-01-21T14:18:41.178379Z","iopub.status.idle":"2022-01-21T14:18:41.215411Z","shell.execute_reply.started":"2022-01-21T14:18:41.178332Z","shell.execute_reply":"2022-01-21T14:18:41.214777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Below, we will find our categorical data types","metadata":{}},{"cell_type":"code","source":"# find categorical variables\n\ncategorical = [var for var in df.columns if df[var].dtype=='O']\n\nprint('There are {} categorical variables\\n'.format(len(categorical)))\n\nprint('The categorical variables are :', categorical)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:41.216362Z","iopub.execute_input":"2022-01-21T14:18:41.21676Z","iopub.status.idle":"2022-01-21T14:18:41.226116Z","shell.execute_reply.started":"2022-01-21T14:18:41.216714Z","shell.execute_reply":"2022-01-21T14:18:41.225112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view the categorical variables\n\nprint(categorical)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:41.227937Z","iopub.execute_input":"2022-01-21T14:18:41.228761Z","iopub.status.idle":"2022-01-21T14:18:41.2424Z","shell.execute_reply.started":"2022-01-21T14:18:41.228695Z","shell.execute_reply":"2022-01-21T14:18:41.241611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:41.244821Z","iopub.execute_input":"2022-01-21T14:18:41.245544Z","iopub.status.idle":"2022-01-21T14:18:41.258475Z","shell.execute_reply.started":"2022-01-21T14:18:41.245508Z","shell.execute_reply":"2022-01-21T14:18:41.257272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for cardinality in categorical variables\n\nfor var in df:\n    print(var, ' contains ', len(df[var].unique()), ' labels')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:41.262211Z","iopub.execute_input":"2022-01-21T14:18:41.263233Z","iopub.status.idle":"2022-01-21T14:18:41.319159Z","shell.execute_reply.started":"2022-01-21T14:18:41.263178Z","shell.execute_reply":"2022-01-21T14:18:41.31821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Below we will find out numerical data types and remove outliers","metadata":{}},{"cell_type":"code","source":"# find numerical variables\n\nnumerical = [var for var in df.columns if df[var].dtype!='O']\n\nprint('There are {} numerical variables\\n'.format(len(numerical)))\n\nprint('The numerical variables are :', numerical)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:41.320667Z","iopub.execute_input":"2022-01-21T14:18:41.320939Z","iopub.status.idle":"2022-01-21T14:18:41.326737Z","shell.execute_reply.started":"2022-01-21T14:18:41.32091Z","shell.execute_reply":"2022-01-21T14:18:41.32615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now we will look for outliers","metadata":{}},{"cell_type":"code","source":"# view summary statistics in numerical variables\n\nprint(round(df.describe()),2)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:41.327875Z","iopub.execute_input":"2022-01-21T14:18:41.328108Z","iopub.status.idle":"2022-01-21T14:18:41.40315Z","shell.execute_reply.started":"2022-01-21T14:18:41.328078Z","shell.execute_reply":"2022-01-21T14:18:41.402243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variables that may contain outliers:\n\n* Population\n* Income\n* Bandwidth_GB_Year","metadata":{}},{"cell_type":"code","source":"# draw boxplots to visualize outliers\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15,10))\n\n\nplt.subplot(2, 2, 1)\nfig = df.boxplot(column='Population')\nfig.set_title('')\nfig.set_ylabel('Population')\n\n\nplt.subplot(2, 2, 2)\nfig = df.boxplot(column='Income')\nfig.set_title('')\nfig.set_ylabel('Income')\n\n\nplt.subplot(2, 2, 3)\nfig = df.boxplot(column='Bandwidth_GB_Year')\nfig.set_title('')\nfig.set_ylabel('Bandwidth_GB_Year')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:41.404468Z","iopub.execute_input":"2022-01-21T14:18:41.404712Z","iopub.status.idle":"2022-01-21T14:18:41.923423Z","shell.execute_reply.started":"2022-01-21T14:18:41.404681Z","shell.execute_reply":"2022-01-21T14:18:41.922476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot histogram to check distribution\n\nplt.figure(figsize=(15,10))\n\n\nplt.subplot(2, 2, 1)\nfig = df.Population.hist(bins=10)\nfig.set_xlabel('Population')\nfig.set_ylabel('Churn')\n\n\nplt.subplot(2, 2, 2)\nfig = df.Income.hist(bins=10)\nfig.set_xlabel('Income')\nfig.set_ylabel('Churn')\n\n\nplt.subplot(2, 2, 3)\nfig = df.Bandwidth_GB_Year.hist(bins=10)\nfig.set_xlabel('Bandwidth_GB_Year')\nfig.set_ylabel('Churn')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:41.924626Z","iopub.execute_input":"2022-01-21T14:18:41.924899Z","iopub.status.idle":"2022-01-21T14:18:42.561475Z","shell.execute_reply.started":"2022-01-21T14:18:41.924867Z","shell.execute_reply":"2022-01-21T14:18:42.560617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing the outliers in our numerical data types\n* Bandwidth_GB_Year does not appear to be skewed.\n* Population and income apprear to be skewed so we will conduct an interquantile range now.","metadata":{}},{"cell_type":"code","source":"# find outliers for Population\n\nIQR = df.Population.quantile(0.75) - df.Population.quantile(0.25)\nlower = df.Population.quantile(0.25) - (IQR * 3)\nupper = df.Population.quantile(0.75) + (IQR * 3)\nprint('Population outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=lower, upperboundary=upper))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:42.563088Z","iopub.execute_input":"2022-01-21T14:18:42.56368Z","iopub.status.idle":"2022-01-21T14:18:42.57704Z","shell.execute_reply.started":"2022-01-21T14:18:42.56363Z","shell.execute_reply":"2022-01-21T14:18:42.576252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find outliers for Income\n\nIQR = df.Income.quantile(0.75) - df.Income.quantile(0.25)\nlower = df.Income.quantile(0.25) - (IQR * 3)\nupper = df.Income.quantile(0.75) + (IQR * 3)\nprint('Income outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=lower, upperboundary=upper))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:42.578448Z","iopub.execute_input":"2022-01-21T14:18:42.578709Z","iopub.status.idle":"2022-01-21T14:18:42.593175Z","shell.execute_reply.started":"2022-01-21T14:18:42.578679Z","shell.execute_reply":"2022-01-21T14:18:42.592375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fixing the outliers in our numerical data types\n\n* We have seen that the Population and Income columns contain outliers. \n* We will use top-coding approach to cap maximum values and remove outliers from the above variables.","metadata":{}},{"cell_type":"code","source":"def max_value(df3, variable, top):\n    return np.where(df3[variable]>top, top, df3[variable])\n\nfor df3 in [df]:\n    df3['Population'] = max_value(df3, 'Population', 50458.0)\n    df3['Income'] = max_value(df3, 'Income', 155310.5275)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:42.594617Z","iopub.execute_input":"2022-01-21T14:18:42.59528Z","iopub.status.idle":"2022-01-21T14:18:42.604979Z","shell.execute_reply.started":"2022-01-21T14:18:42.595234Z","shell.execute_reply":"2022-01-21T14:18:42.604018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.Population.max(), df.Income.max())","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:42.606333Z","iopub.execute_input":"2022-01-21T14:18:42.606825Z","iopub.status.idle":"2022-01-21T14:18:42.625226Z","shell.execute_reply.started":"2022-01-21T14:18:42.606777Z","shell.execute_reply":"2022-01-21T14:18:42.624429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot histogram to check distribution of removed outliers \n\nplt.figure(figsize=(15,10))\n\n\nplt.subplot(2, 2, 1)\nfig = df.Population.hist(bins=10)\nfig.set_xlabel('Population')\nfig.set_ylabel('Churn')\n\n\nplt.subplot(2, 2, 2)\nfig = df.Income.hist(bins=10)\nfig.set_xlabel('Income')\nfig.set_ylabel('Churn')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:42.626339Z","iopub.execute_input":"2022-01-21T14:18:42.627003Z","iopub.status.idle":"2022-01-21T14:18:43.07645Z","shell.execute_reply.started":"2022-01-21T14:18:42.626959Z","shell.execute_reply":"2022-01-21T14:18:43.075332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### C4.  Provide a copy of the cleaned data set.","metadata":{}},{"cell_type":"code","source":"# Desired data set\ndf.to_csv('Decision_Tree_churn.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:43.078275Z","iopub.execute_input":"2022-01-21T14:18:43.078693Z","iopub.status.idle":"2022-01-21T14:18:43.272174Z","shell.execute_reply.started":"2022-01-21T14:18:43.078644Z","shell.execute_reply":"2022-01-21T14:18:43.271036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part IV: Analysis","metadata":{}},{"cell_type":"markdown","source":"### D1.  Split the data into training and test data sets and provide the file(s).","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Create arrays for the features and the response variable\n\ntrain, test = train_test_split(df, test_size = 0.2, random_state = 0)\n\n# check the shape of X_train and X_test\n\ntrain.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:43.274974Z","iopub.execute_input":"2022-01-21T14:18:43.275309Z","iopub.status.idle":"2022-01-21T14:18:43.288508Z","shell.execute_reply.started":"2022-01-21T14:18:43.275265Z","shell.execute_reply":"2022-01-21T14:18:43.287813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put training and test data into their own CSVs.\n\ntrain.to_csv('training_churn.csv', index=False)\n\ntest.to_csv('test_churn.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:43.289844Z","iopub.execute_input":"2022-01-21T14:18:43.290273Z","iopub.status.idle":"2022-01-21T14:18:43.485646Z","shell.execute_reply.started":"2022-01-21T14:18:43.290221Z","shell.execute_reply":"2022-01-21T14:18:43.484955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now that we have split the test/training data, we will split the dependent variable \"Churn\" from the independent variables.","metadata":{}},{"cell_type":"code","source":"# create target(predictor) variable \n\nX = df.drop(['Churn'], axis=1)\n\ny = df[['Churn']]","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:43.486991Z","iopub.execute_input":"2022-01-21T14:18:43.487393Z","iopub.status.idle":"2022-01-21T14:18:43.495271Z","shell.execute_reply.started":"2022-01-21T14:18:43.487346Z","shell.execute_reply":"2022-01-21T14:18:43.494501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing churn from categorical list because we will loop through the categorical in the encoding below\n\ncategorical.remove('Churn')\n\ncategorical","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:43.499515Z","iopub.execute_input":"2022-01-21T14:18:43.500375Z","iopub.status.idle":"2022-01-21T14:18:43.507823Z","shell.execute_reply.started":"2022-01-21T14:18:43.500269Z","shell.execute_reply":"2022-01-21T14:18:43.506919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting test/training data\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n\n# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:43.50874Z","iopub.execute_input":"2022-01-21T14:18:43.509559Z","iopub.status.idle":"2022-01-21T14:18:43.527956Z","shell.execute_reply.started":"2022-01-21T14:18:43.509508Z","shell.execute_reply":"2022-01-21T14:18:43.527231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"churn_df = y_test\nchurn_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:43.530966Z","iopub.execute_input":"2022-01-21T14:18:43.531241Z","iopub.status.idle":"2022-01-21T14:18:43.541107Z","shell.execute_reply.started":"2022-01-21T14:18:43.531208Z","shell.execute_reply":"2022-01-21T14:18:43.540067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn.preprocessing as preprocessing\n\n# transforming categorical datatypes into numerical types\nfor feature in categorical:\n    le = preprocessing.LabelEncoder()\n    X_test.loc[:, feature] = le.fit_transform(X_test.loc[:, feature])\n    X_train.loc[:, feature] = le.fit_transform(X_train.loc[:, feature])\ny_test = le.fit_transform(y_test)\ny_train = le.fit_transform(y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:43.542233Z","iopub.execute_input":"2022-01-21T14:18:43.542934Z","iopub.status.idle":"2022-01-21T14:18:43.937891Z","shell.execute_reply.started":"2022-01-21T14:18:43.542898Z","shell.execute_reply":"2022-01-21T14:18:43.936904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# normalizaing/feature scaling the data\n\nscaler = StandardScaler()\n\nX_test = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:43.939283Z","iopub.execute_input":"2022-01-21T14:18:43.939601Z","iopub.status.idle":"2022-01-21T14:18:43.967776Z","shell.execute_reply.started":"2022-01-21T14:18:43.93956Z","shell.execute_reply":"2022-01-21T14:18:43.966798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### D2: Analysis and Intermediate calculations\n    * The analysis technique we are using it is the Decision tree and more specifically the Decision tree regressor model\n        * With this model, it will start at a root node (the initial feature with the highest importance)\n        * From the root node (which turns into the parent node) splits into the other features (called child nodes at this point or leafs) based on the decision made from the customer data\n        * Resulting in the classifcation of customer churn","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\n# Declare Decision Tree algo \ndt = DecisionTreeRegressor(max_depth = 8,\n                              min_samples_leaf = 0.1,\n                              random_state = 1)\n# Fit the model\ndt.fit(X_train, y_train)\n\n# Declare prediction variable\ny_pred = dt.predict(X_test)\n\ny_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:43.969031Z","iopub.execute_input":"2022-01-21T14:18:43.969292Z","iopub.status.idle":"2022-01-21T14:18:44.013225Z","shell.execute_reply.started":"2022-01-21T14:18:43.969261Z","shell.execute_reply":"2022-01-21T14:18:44.012185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.datasets import make_regression\n\n# define dataset\nX, y = make_regression(n_samples=10000, n_features=len(X_train.columns), n_informative=5, random_state=1)\n\n# get importance\nimportance = dt.feature_importances_\n\n# summarize feature importance\nfor i,feature_score in enumerate(importance):\n    print((X_train.columns[i]), '- %.5f' % (feature_score))\n    \n# plot feature importance\nplt.figure(figsize=(10,10))\nplt.bar([x for x in range(len(importance))], importance)\nplt.xlabel('Feature')\nplt.ylabel('Score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:44.014776Z","iopub.execute_input":"2022-01-21T14:18:44.015599Z","iopub.status.idle":"2022-01-21T14:18:44.368364Z","shell.execute_reply.started":"2022-01-21T14:18:44.015539Z","shell.execute_reply":"2022-01-21T14:18:44.367282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features of importance\n\nfor i,feature_score in enumerate(importance):\n    if feature_score > 0.0001:\n        print((X_train.columns[i]), '- %.5f' % (feature_score))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:44.369711Z","iopub.execute_input":"2022-01-21T14:18:44.369975Z","iopub.status.idle":"2022-01-21T14:18:44.376526Z","shell.execute_reply.started":"2022-01-21T14:18:44.369945Z","shell.execute_reply":"2022-01-21T14:18:44.375515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = X_test\nY = y_test","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:44.377908Z","iopub.execute_input":"2022-01-21T14:18:44.378174Z","iopub.status.idle":"2022-01-21T14:18:44.388924Z","shell.execute_reply.started":"2022-01-21T14:18:44.378139Z","shell.execute_reply":"2022-01-21T14:18:44.388222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### D3.  Code is above.","metadata":{}},{"cell_type":"markdown","source":"## Part V: Data Summary and Implications\n\n### E1.  Accuracy and the mean squared error (MSE)","metadata":{}},{"cell_type":"code","source":"# Import cross validation metrics\nfrom sklearn.model_selection import cross_val_score\n\n# Compute the coefficient of determination (R-squared)\nscores = cross_val_score(dt, X, y, scoring='r2')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:44.390176Z","iopub.execute_input":"2022-01-21T14:18:44.390598Z","iopub.status.idle":"2022-01-21T14:18:44.891956Z","shell.execute_reply.started":"2022-01-21T14:18:44.390566Z","shell.execute_reply":"2022-01-21T14:18:44.890635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print R-squared value\nprint('Cross validation R-squared values: ', scores)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:44.893204Z","iopub.execute_input":"2022-01-21T14:18:44.893427Z","iopub.status.idle":"2022-01-21T14:18:44.898952Z","shell.execute_reply.started":"2022-01-21T14:18:44.8934Z","shell.execute_reply":"2022-01-21T14:18:44.897912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.metrics import mean_squared_error as MSE\n\nprint(\"Best Possible score for R-Squared is 1\")\n\n# Explained variance of the training set\ndt.fit(X_train,y_train)\nprint(\"R-Squared on training dataset = {}\".format(dt.score(X_test,y_test)))\n\n# Explained variance of the test set\ndt.fit(X_test,y_test)\nprint(\"R-Squared on test dataset = {}\\n\".format(dt.score(X_test,y_test)))\n\n\nprint(\"Best Possible score for MSE is 0\")\n\n\n# Print Mean Squared Error\nprint(\"MSE = \", MSE(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:44.900702Z","iopub.execute_input":"2022-01-21T14:18:44.901157Z","iopub.status.idle":"2022-01-21T14:18:44.959694Z","shell.execute_reply.started":"2022-01-21T14:18:44.901108Z","shell.execute_reply":"2022-01-21T14:18:44.958555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Mean squared error is approximately 0.1122\n * 0 would be perfect, so this is a pretty accurate model","metadata":{}},{"cell_type":"code","source":"reduced_X_train = pd.DataFrame({'Contract': X_train['Contract'], 'Tenure': X_train['Tenure'], 'MonthlyCharge': X_train['MonthlyCharge']})\nreduced_X_test = pd.DataFrame({'Contract': X_test['Contract'], 'Tenure': X_test['Tenure'], 'MonthlyCharge': X_test['MonthlyCharge']})","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:44.962898Z","iopub.execute_input":"2022-01-21T14:18:44.963151Z","iopub.status.idle":"2022-01-21T14:18:44.970337Z","shell.execute_reply.started":"2022-01-21T14:18:44.963121Z","shell.execute_reply":"2022-01-21T14:18:44.969351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt.fit(reduced_X_train, y_train)\ny_pred = dt.predict(reduced_X_test)\ny_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:44.971948Z","iopub.execute_input":"2022-01-21T14:18:44.972288Z","iopub.status.idle":"2022-01-21T14:18:44.996817Z","shell.execute_reply.started":"2022-01-21T14:18:44.972243Z","shell.execute_reply":"2022-01-21T14:18:44.995632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.DataFrame({'Contract': X_test['Contract'], 'Tenure': X_test['Tenure'], 'MonthlyCharge': X_test['MonthlyCharge'],'Churn': y_pred})\ndata","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:44.998042Z","iopub.execute_input":"2022-01-21T14:18:44.998799Z","iopub.status.idle":"2022-01-21T14:18:45.015208Z","shell.execute_reply.started":"2022-01-21T14:18:44.998758Z","shell.execute_reply":"2022-01-21T14:18:45.014466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_features = pd.DataFrame({'Contract': X_test['Contract'], 'Tenure': X_test['Tenure'], 'MonthlyCharge': X_test['MonthlyCharge']})\nlen(data_features)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:45.016403Z","iopub.execute_input":"2022-01-21T14:18:45.016641Z","iopub.status.idle":"2022-01-21T14:18:45.028889Z","shell.execute_reply.started":"2022-01-21T14:18:45.01661Z","shell.execute_reply":"2022-01-21T14:18:45.027845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explained variance after model reduction\nprint(\"Best Possible score for R-Squared is 1\")\n\ndt.fit(data[['Contract', 'Tenure', 'MonthlyCharge']], data['Churn'])\nprint(\"R-Squared on training dataset = {}\\n\".format(dt.score(data[['Contract', 'Tenure', 'MonthlyCharge']], data['Churn'])))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:45.030088Z","iopub.execute_input":"2022-01-21T14:18:45.030409Z","iopub.status.idle":"2022-01-21T14:18:45.05342Z","shell.execute_reply.started":"2022-01-21T14:18:45.030362Z","shell.execute_reply":"2022-01-21T14:18:45.052524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parameters of Decision tree regression model\ndt.get_params()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:45.054619Z","iopub.execute_input":"2022-01-21T14:18:45.055112Z","iopub.status.idle":"2022-01-21T14:18:45.064097Z","shell.execute_reply.started":"2022-01-21T14:18:45.055075Z","shell.execute_reply":"2022-01-21T14:18:45.063394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:45.065059Z","iopub.execute_input":"2022-01-21T14:18:45.065312Z","iopub.status.idle":"2022-01-21T14:18:45.082523Z","shell.execute_reply.started":"2022-01-21T14:18:45.065269Z","shell.execute_reply":"2022-01-21T14:18:45.081389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"churn_df.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:45.084186Z","iopub.execute_input":"2022-01-21T14:18:45.085174Z","iopub.status.idle":"2022-01-21T14:18:45.09965Z","shell.execute_reply.started":"2022-01-21T14:18:45.085127Z","shell.execute_reply":"2022-01-21T14:18:45.098791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Below removes all customers that have already discontinued their services in our sample size","metadata":{}},{"cell_type":"code","source":"i = 0\nchurn_likelihood = []\nchurn_cust = []\nfor cust in churn_df['Churn']:\n    if cust == \"No\":\n        churn_likelihood.append(y_pred[i])\n    i += 1\nprint(len(churn_likelihood))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:45.101292Z","iopub.execute_input":"2022-01-21T14:18:45.101822Z","iopub.status.idle":"2022-01-21T14:18:45.11494Z","shell.execute_reply.started":"2022-01-21T14:18:45.101776Z","shell.execute_reply":"2022-01-21T14:18:45.113892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"very_low = 0\nlow = 0\nmedium = 0\nhigh = 0\nvery_high = 0\n\nfor customer in churn_likelihood:\n    if customer <= .03:\n        very_low += 1\n    elif customer > .03 and customer <= .05:\n        low += 1\n    elif customer > .05 and customer <= .08:\n        medium += 1\n    elif customer > .08 and customer <= .095:\n        high += 1\n    elif customer > .095:\n        very_high += 1\n        \nprint(\"Likelihood of customer churn:\\n\", \n      very_low, \"customers have very low risk\\n\",\n      low, \"customers have low risk\\n\",\n      medium, \"customers have medium risk\\n\",\n      high, \"customers have high risk\\n\",\n      very_high, \"customers have very high risk\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T14:18:45.792659Z","iopub.execute_input":"2022-01-21T14:18:45.793534Z","iopub.status.idle":"2022-01-21T14:18:45.808744Z","shell.execute_reply.started":"2022-01-21T14:18:45.793488Z","shell.execute_reply":"2022-01-21T14:18:45.807701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.  Results and implications\n\n    * From decision tree regression technique, we found out that the main variables of the customer churn are:\n        * The type of contract the customer was in\n        * The amount of years the customer had already been using the service\n        * The amount of money they were being charged monthly\n        \n    * From a sample size of 2000 customers, we found that 438 customers were at high risk of churn as identified from our model.\n    \n    * The implications that occured during analysis consisted mostly of having to many \"branches\". Our decision tree started with 37 branches and after \"pruning\" the tree were were left with only 3 that gave us a solid R-squared value/accuracy.\n\n3.  Limitation\n\n    * The major limitation that occured during analysis was amount of observations that I had to work with. A 80/20 split was chosen for the train/test data split which gives our model a 8000 observations to train from and 2000 to test on. The 2000 observations tested on are essentially our sample size for the results. We could have did a different split but the model would have been less trained.\n\n4.  Recommendation\n\n    * The recommendation for telecommunications comapany is to ensure that the needs of all customers are taken care of but really focus on the customers with high tenures, identify what kind of contract has the highest longevity and perhaps suggest that contract type to more customers, and take a closer look to see what price range for the monthly bill has the best rate of satisfaction with the customer base as the monthly charge is the second most important variable with customers.\n \n\nPart VI: Demonstration\n\nF.  Panopto video:\n \n    https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=19c5726f-2558-4948-83f3-ae23016249fd\n\nG.  Third Party Code sources.\n    \n    Brownlee, J. (2020). How to Calculate Feature Importance With Python. Machine Learning Mastery. https://machinelearningmastery.com/calculate-feature-importance-with-python/\n\nH.  References:\n    \n    2U INC. (2022). Decision Tree. Master's in Data Science. https://www.mastersindatascience.org/learning/introduction-to-machine-learning-algorithms/decision-tree/\n    \n    Chauhan, N. (2020). Decision Tree Algorithm, Explained. KDnuggets. https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html\n","metadata":{}}]}